{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0404dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb13777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /aiffel/tensorflow_datasets/glue/mnli/2.0.0\n",
      "INFO:absl:Reusing dataset glue (/aiffel/tensorflow_datasets/glue/mnli/2.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset glue for split None, from /aiffel/tensorflow_datasets/glue/mnli/2.0.0\n"
     ]
    }
   ],
   "source": [
    "data, info = tfds.load('glue/mnli', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b8c21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', shape=(), dtype=string)\n",
      "tf.Tensor(b'Meaningful partnerships with stakeholders is crucial.', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "examples = data['train'].take(1)\n",
    "for example in examples:\n",
    "    sentence1 = example['premise']\n",
    "    sentence2 = example['hypothesis']\n",
    "    label = example['label']\n",
    "    print(sentence1)\n",
    "    print(sentence2)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98610df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MNLI data set (GLUE version).\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        # print(tensor_dict)\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy())\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(saelf._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")), \"dev_matched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test_matched\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\"] # ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = f\"{set_type}-{line[0]}\"\n",
    "            text_a = line[8]\n",
    "            text_b = line[9]\n",
    "            label = None if set_type.startswith(\"test\") else line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24969663",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MnliProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca9cb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc268716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb0a96f32d84c3996662963b6e8ddfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e4ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"claasification\") :\n",
    "    if max_length is None :\n",
    "        max_length = tokenizer.max_len\n",
    "    if label_list is None:\n",
    "        label_list = processor.get_labels()\n",
    "        print(\"Using label list %s\" % (label_list))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    labels = [label_map[example.label] for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "\n",
    "        feature = InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "613658df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"classification\") :\n",
    "    \"\"\"\n",
    "    :param examples: tf.data.Dataset\n",
    "    :param tokenizer: pretrained tokenizer\n",
    "    :param max_length: example의 최대 길이(기본값 : tokenizer의 max_len)\n",
    "    :param task: GLUE task 이름\n",
    "    :param label_list: 라벨 리스트\n",
    "    :param output_mode: \"regression\" or \"classification\"\n",
    "\n",
    "    :return: task에 맞도록 feature가 구성된 tf.data.Dataset\n",
    "    \"\"\"\n",
    "    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n",
    "    features = _glue_convert_examples_to_features(examples, tokenizer, max_length, processor)\n",
    "    label_type = tf.int64\n",
    "\n",
    "    def gen():\n",
    "        for ex in features:\n",
    "            d = {k: v for k, v in asdict(ex).items() if v is not None}\n",
    "            label = d.pop(\"label\")\n",
    "            yield (d, label)\n",
    "\n",
    "    input_names = [\"input_ids\"] + tokenizer.model_input_names\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({k: tf.int32 for k in input_names}, label_type),\n",
    "        ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c6d2054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "Using label list ['0', '1', '2']\n",
      "Using label list ['0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)\n",
    "train_dataset_batch = train_dataset.shuffle(100).batch(16).repeat(2)\n",
    "\n",
    "validation_dataset = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=128, processor=processor)\n",
    "validation_dataset_batch = validation_dataset.shuffle(100).batch(16)\n",
    "\n",
    "test_dataset = tf_glue_convert_examples_to_features(data['test_matched'], tokenizer, max_length=128, processor=processor)\n",
    "test_dataset_batch = test_dataset.shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d3bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(processor.get_labels())\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc77a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e619a595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "115/115 [==============================] - 152s 1s/step - loss: 1.0010 - acc: 0.4951 - val_loss: 0.8838 - val_acc: 0.6062\n",
      "Epoch 2/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.8473 - acc: 0.6185 - val_loss: 0.7657 - val_acc: 0.6673\n",
      "Epoch 3/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.7901 - acc: 0.6620 - val_loss: 0.7459 - val_acc: 0.6743\n",
      "Epoch 4/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.7317 - acc: 0.6908 - val_loss: 0.6938 - val_acc: 0.7115\n",
      "Epoch 5/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.7096 - acc: 0.7043 - val_loss: 0.6816 - val_acc: 0.7135\n",
      "Epoch 6/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.7540 - acc: 0.6772 - val_loss: 0.6486 - val_acc: 0.7346\n",
      "Epoch 7/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6743 - acc: 0.7158 - val_loss: 0.6523 - val_acc: 0.7275\n",
      "Epoch 8/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6704 - acc: 0.7120 - val_loss: 0.6391 - val_acc: 0.7356\n",
      "Epoch 9/20\n",
      "115/115 [==============================] - 139s 1s/step - loss: 0.6312 - acc: 0.7484 - val_loss: 0.6392 - val_acc: 0.7423\n",
      "Epoch 10/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6260 - acc: 0.7418 - val_loss: 0.6132 - val_acc: 0.7524\n",
      "Epoch 11/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6501 - acc: 0.7402 - val_loss: 0.6220 - val_acc: 0.7478\n",
      "Epoch 12/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6429 - acc: 0.7418 - val_loss: 0.6026 - val_acc: 0.7494\n",
      "Epoch 13/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6202 - acc: 0.7413 - val_loss: 0.6010 - val_acc: 0.7558\n",
      "Epoch 14/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6070 - acc: 0.7478 - val_loss: 0.5965 - val_acc: 0.7624\n",
      "Epoch 15/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.5946 - acc: 0.7543 - val_loss: 0.5762 - val_acc: 0.7720\n",
      "Epoch 16/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6286 - acc: 0.7342 - val_loss: 0.5817 - val_acc: 0.7667\n",
      "Epoch 17/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6017 - acc: 0.7533 - val_loss: 0.5636 - val_acc: 0.7741\n",
      "Epoch 18/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6006 - acc: 0.7451 - val_loss: 0.5671 - val_acc: 0.7788\n",
      "Epoch 19/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.6037 - acc: 0.7500 - val_loss: 0.5581 - val_acc: 0.7816\n",
      "Epoch 20/20\n",
      "115/115 [==============================] - 138s 1s/step - loss: 0.5950 - acc: 0.7685 - val_loss: 0.5579 - val_acc: 0.7780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a10f20310>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset_batch, epochs=20, steps_per_epoch=115, \n",
    "          validation_data=validation_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9573fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 92s 150ms/step - loss: 1.9799 - acc: 0.3544\n",
      "[1.9799156188964844, 0.3544303774833679]\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "result = model.evaluate(test_dataset_batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cf798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
